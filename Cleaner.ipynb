{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were going to imputate several well logs data with model that weh have trained and imported to classified the lithology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First importing all the necessary libraries and dataframe we want to imiputate, and do some light cleaning like replacing the extreme negative values and turn our dataset to numeric values before we imputate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('7_1-1.csv') #Adjust with the csv you want to clean\n",
    "df.iloc[1:].reset_index(drop=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "print(df.info())\n",
    "print(df.head(1))\n",
    "#df['RXO'].unique()\n",
    "#df.loc[(df['RXO'] < -200.0) & (df['RXO'] > -999.9999), 'RXO'] = np.nan\n",
    "#df['RXO'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There some well logs that have missing feature for the model we create, the empty colum to predict the value also. and we seperate DEPT and filtering dataset with only features used on imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['RSHA'] = np.nan #Adjust Wich feature is missing \n",
    "df['RXO'] = np.nan #Adjust Wich feature is missing \n",
    "filtered_model = ['GR', 'RHOB', 'NPHI', 'DTC', 'RDEP', 'RXO', 'RSHA', 'FORCE_2020_LITHOFACIES_LITHOLOGY'] #Adjust with the columns you want to impute\n",
    "df_filtered_model = df[filtered_model]\n",
    "other_columns = df.drop(columns=filtered_model)\n",
    "other_columns.info()\n",
    "df_filtered_model.info()\n",
    "df_filtered_model_imputed = df_filtered_model.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the model we have trained for imputation. and imputate each missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load models\n",
    "models = {\n",
    "    'GR': joblib.load('GR_regressor.pkl'),\n",
    "    'RHOB': joblib.load('RHOB_regressor.pkl'),\n",
    "    'NPHI': joblib.load('NPHI_regressor.pkl'),\n",
    "    'DTC': joblib.load('DTC_regressor.pkl'),\n",
    "    'RDEP': joblib.load('RDEP_regressor.pkl'),\n",
    "    'RXO': joblib.load('RXO_regressor.pkl'),\n",
    "    'RSHA': joblib.load('RSHA_regressor.pkl')\n",
    "}\n",
    "def impute_column(df, column_name, model):\n",
    "    missing_mask = df[column_name].isna()\n",
    "    if missing_mask.sum() > 0:  # Check if there are missing values to impute\n",
    "        missing_data = df[missing_mask]\n",
    "        if missing_data.shape[0] > 0:\n",
    "            features = missing_data.drop(columns=[column_name])\n",
    "            # Ensure that features do not contain NaNs\n",
    "            features = features.fillna(features.median())\n",
    "            predicted_values = model.predict(features)\n",
    "            df.loc[missing_mask, column_name] = predicted_values\n",
    "        else:\n",
    "            print(f\"No missing data found for column '{column_name}'.\")\n",
    "    else:\n",
    "        print(f\"No missing values to impute for column '{column_name}'.\")\n",
    "# Impute missing values\n",
    "for column in models.keys():\n",
    "    if column in df_filtered_model_imputed.columns:\n",
    "        impute_column(df_filtered_model_imputed, column, models[column])\n",
    "# Print the results\n",
    "print(\"First few rows of the imputed DataFrame:\")\n",
    "print(df_filtered_model_imputed.head())\n",
    "\n",
    "print(\"\\nSummary statistics of the DataFrame:\")\n",
    "print(df_filtered_model_imputed.describe())\n",
    "\n",
    "print(\"\\nCount of missing values in each column:\")\n",
    "print(df_filtered_model_imputed.isna().sum())\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df_filtered_model_imputed.to_csv('{7_1-1_imputed.csv', index=False)\n",
    "print(\"\\nDataFrame with imputed values saved to '7_1-1_imputed.csv'.\")\n",
    "df_filtered_model_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_model_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation Result is combined again with the dept value to gave information about the dept for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.concat([other_columns, df_filtered_model_imputed], axis=1)\n",
    "df_result.info()\n",
    "df_result.to_csv('{7_1-1_imputed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
